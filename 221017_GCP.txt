Stateful
Stateless

Client >Web Browser	
	Internet (Network)=>
			Server > WAS > file > html

Network는 상호 연결 가능하나 각각 격리되어있음
	Network에 Gateway를 생성해 클라이언트가 접속할 수 있도록 함

클라이언트가 게이트웨이로 서버에 접속하면 서버쪽에서 트래픽을 보내 VM인스터스로 서비스가 가능하게 함

VM인스터스는 정보저장을하고 클라이언트에겐 최초의 페이지 정도만 제공

Nat Gateway 

Cloud Storage

block Storage - Oracle DB / Chunk data
File Storage
Object Storage


구성
 bucket - 데이터를 담는 최초의 단위
 	전역적으로 고유한 이름으로 설정
		URL주소를 통해 데이터를 찾기 때문에 중복이름 불가

 storage class - Storage in the Cloud.pdf 7page


실습
 1. 다중VPC 네트워크

작업 1. 방화벽 규칙이 있는 커스텀 모드 VPC 네트워크 만들기
SSH, ICMP 및 RDP 인그레스 트래픽을 허용하도록 방화벽 규칙과 함께 2개의 커스텀 네트워크(managementnet 및 privatenet)를 만듭니다.

managementnet 네트워크 만들기
Cloud 콘솔을 사용하여 managementnet 네트워크를 만듭니다.

Cloud 콘솔에서 탐색 메뉴(탐색 메뉴 아이콘) > VPC 네트워크 > VPC 네트워크로 이동합니다.
탐색 메뉴

기본 네트워크와 mynetwork 네트워크에 서브넷이 있는 것을 확인할 수 있습니다.

각 Google Cloud 프로젝트는 기본 네트워크로 시작합니다. 또한 mynetwork 네트워크는 네트워크 다이어그램의 일부로 사전에 만들어져 있습니다.

VPC 네트워크 만들기를 클릭합니다.

이름을 managementnet으로 설정합니다.

서브넷 생성 모드의 경우 커스텀을 클릭합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
이름	managementsubnet-us
리전	us-east1
IPv4 범위	10.130.0.0/20
완료를 클릭합니다.

상응하는 명령줄을 클릭합니다.

이러한 명령어는 Cloud Shell 명령줄을 사용하여 네트워크와 서브넷을 만들 수 있음을 보여줍니다. 유사한 매개변수와 함께 이러한 명령어를 사용하여 privatenet 네트워크를 만들게 됩니다.

닫기를 클릭합니다.

만들기를 클릭합니다.

완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. managementnet 네트워크가 정상적으로 생성되면 평가 점수가 표시됩니다.

managementnet 네트워크 만들기
privatenet 네트워크 만들기
Cloud Shell 명령줄을 사용하여 privatenet 네트워크를 만듭니다.

다음 명령어를 실행하여 privatenet 네트워크를 만듭니다.

gcloud compute networks create privatenet --subnet-mode=custom
복사되었습니다.
다음 명령어를 실행하여 privatesubnet-us 서브넷을 만듭니다.

gcloud compute networks subnets create privatesubnet-us --network=privatenet --region=us-east1 --range=172.16.0.0/24
복사되었습니다.
다음 명령어를 실행하여 privatesubnet-eu 서브넷을 만듭니다.

gcloud compute networks subnets create privatesubnet-eu --network=privatenet --region=europe-west4 --range=172.20.0.0/20
복사되었습니다.
완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. privatenet 네트워크가 정상적으로 생성되면 평가 점수가 표시됩니다.

privatenet 네트워크 만들기
다음 명령어를 실행하여 사용 가능한 VPC 네트워크의 목록을 출력합니다.

gcloud compute networks list
복사되었습니다.
출력은 다음과 같습니다.

NAME: default
SUBNET_MODE: AUTO
BGP_ROUTING_MODE: REGIONAL
IPV4_RANGE:
GATEWAY_IPV4:
NAME: managementnet
SUBNET_MODE: CUSTOM
BGP_ROUTING_MODE: REGIONAL
IPV4_RANGE:
GATEWAY_IPV4:
...
참고: default 및 mynetwork는 자동 모드 네트워크이고 managementnet 및 privatenet은 커스텀 모드 네트워크입니다. 자동 모드 네트워크를 사용하면 각 리전에 자동으로 서브넷이 만들어지는 반면, 커스텀 모드 네트워크는 서브넷 없이 시작되므로 서브넷 생성을 전적으로 제어할 수 있습니다.
다음 명령어를 실행하여 사용 가능한 VPC 서브넷의 목록을 출력합니다(VPC 네트워크 기준으로 정렬).

gcloud compute networks subnets list --sort-by=NETWORK
복사되었습니다.
출력은 다음과 같습니다.

NAME: default
REGION: us-east1
NETWORK: default
RANGE: 10.128.0.0/20
STACK_TYPE: IPV4_ONLY
IPV6_ACCESS_TYPE:
INTERNAL_IPV6_PREFIX:
EXTERNAL_IPV6_PREFIX:
...
참고: 예상대로 default 및 mynetwork 네트워크는 자동 모드 네트워크이므로 각 리전에 서브넷이 있습니다. managementnet 및 privatenet 네트워크는 커스텀 모드 네트워크이므로 직접 생성한 서브넷만 있습니다.
Cloud 콘솔에서 탐색 메뉴 > VPC 네트워크 > VPC 네트워크로 이동합니다.

Cloud 콘솔에 동일한 네트워크와 서브넷이 나열됩니다.

managementnet의 방화벽 규칙 만들기
managementnet 네트워크의 VM 인스턴스에 대한 SSH, ICMP, RDP 인그레스 트래픽을 허용하는 방화벽 규칙을 만듭니다.

Cloud 콘솔에서 탐색 메뉴(탐색 메뉴 아이콘) > VPC 네트워크 > 방화벽으로 이동합니다.

+ 방화벽 규칙 만들기를 클릭합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
이름	managementnet-allow-icmp-ssh-rdp
네트워크	managementnet
대상	네트워크의 모든 인스턴스
소스 필터	IPv4 범위
소스 IPv4 범위	0.0.0.0/0
프로토콜 및 포트	지정된 프로토콜 및 포트, 그리고 check tcp, type: 22, 3389. 그리고 check 기타 프로토콜, type: icmp.
참고: 소스 IPv4 범위에 /0을 입력하여 모든 네트워크를 지정할 수 있도록 합니다.
상응하는 명령줄을 클릭합니다.

이러한 명령어는 Cloud Shell 명령줄을 사용하여 방화벽 규칙을 만들 수도 있음을 보여줍니다. 유사한 매개변수와 함께 이러한 명령어를 사용하여 privatenet의 방화벽 규칙을 만들게 됩니다.

닫기를 클릭합니다.

만들기를 클릭합니다.

완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. managementnet 네트워크의 방화벽 규칙이 정상적으로 생성되면 평가 점수가 표시됩니다.

managementnet의 방화벽 규칙 만들기
privatenet의 방화벽 규칙 만들기
Cloud Shell 명령줄을 사용하여 privatenet 네트워크의 방화벽 규칙을 만듭니다.

Cloud Shell에서 다음 명령어를 실행하여 privatenet-allow-icmp-ssh-rdp 방화벽 규칙을 만듭니다.

gcloud compute firewall-rules create privatenet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=privatenet --action=ALLOW --rules=icmp,tcp:22,tcp:3389 --source-ranges=0.0.0.0/0
복사되었습니다.
출력은 다음과 같습니다.

Creating firewall...done.
NAME: privatenet-allow-icmp-ssh-rdp
NETWORK: privatenet
DIRECTION: INGRESS
PRIORITY: 1000
ALLOW: icmp,tcp:22,tcp:3389
DENY:
DISABLED: False
완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. privatenet 네트워크의 방화벽 규칙이 정상적으로 생성되면 평가 점수가 표시됩니다.

privatenet의 방화벽 규칙 만들기
다음 명령어를 실행하여 모든 방화벽 규칙의 목록을 출력합니다(VPC 네트워크 기준으로 정렬).

gcloud compute firewall-rules list --sort-by=NETWORK
복사되었습니다.
출력은 다음과 같습니다.

NAME: default-allow-icmp
NETWORK: default
DIRECTION: INGRESS
PRIORITY: 65534
ALLOW: icmp
DENY:
DISABLED: False
NAME: default-allow-internal
NETWORK: default
DIRECTION: INGRESS
PRIORITY: 65534
ALLOW: tcp:0-65535,udp:0-65535,icmp
DENY:
DISABLED: False
...
mynetwork 네트워크의 방화벽 규칙은 이미 생성되어 있습니다. 여러 개의 프로토콜과 포트를 하나의 방화벽 규칙(privatenet 및 managementnet)에 정의하거나 여러 규칙(기본 및 mynetwork)에 분산시킬 수 있습니다.

Cloud 콘솔에서 탐색 메뉴 > VPC 네트워크 > 방화벽으로 이동합니다.

Cloud 콘솔에도 동일한 방화벽 규칙이 표시됩니다.

작업 2. VM 인스턴스 만들기
다음과 같은 2개의 VM 인스턴스를 만듭니다.

managementsubnet-us의 managementnet-us-vm

privatesubnet-us에 privatenet-us-vm

managementnet-us-vm 인스턴스 만들기
Cloud 콘솔을 사용하여 managementnet-us-vm 인스턴스를 만듭니다.

Cloud 콘솔에서 탐색 메뉴 > Compute Engine > VM 인스턴스로 이동합니다.

mynet-eu-vm 및 mynet-us-vm이 네트워크 다이어그램의 일부로 이미 생성되어 있습니다.

인스턴스 만들기를 클릭합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
이름	managementnet-us-vm
리전	us-east1
영역	<filled in at lab start>
시리즈	E2
머신 유형	e2-micro
고급 옵션에서 네트워킹, 디스크, 보안, 관리, 단독 테넌시 드롭다운을 클릭합니다.

네트워킹을 클릭합니다.

네트워크 인터페이스에서 드롭다운을 클릭하여 수정합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
네트워크	managementnet
서브네트워크	managementsubnet-us
완료를 클릭합니다.

상응하는 명령줄을 클릭합니다.

gcloud compute instances create managementnet-us-vm --project=qwiklabs-gcp-01-e4037ca915c3 --zone=us-east1-b --machine-type=e2-micro --network-interface=network-tier=PREMIUM,subnet=managementsubnet-us --metadata=enable-oslogin=true --maintenance-policy=MIGRATE --provisioning-model=STANDARD --service-account=483568669459-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --create-disk=auto-delete=yes,boot=yes,device-name=managementnet-us-vm,image=projects/debian-cloud/global/images/debian-11-bullseye-v20220920,mode=rw,size=10,type=projects/qwiklabs-gcp-01-e4037ca915c3/zones/us-east1-b/diskTypes/pd-balanced --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --reservation-affinity=any


이는 Cloud Shell 명령줄을 사용하여 VM 인스턴스를 만들 수도 있음을 보여줍니다. 유사한 매개변수와 함께 이러한 명령어를 사용하여 privatenet-us-vm 인스턴스를 만들게 됩니다.

닫기를 클릭합니다.

만들기를 클릭합니다.

완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. managementnet 네트워크에 VM 인스턴스가 정상적으로 생성되면 평가 점수가 표시됩니다.

managementnet-us-vm 인스턴스 만들기
privatenet-us-vm 인스턴스 만들기
Cloud Shell 명령줄을 사용하여 privatenet-us-vm 인스턴스를 만듭니다.

Cloud Shell에서 다음 명령어를 사용하여 privatenet-us-vm 인스턴스를 만듭니다.
gcloud compute instances create privatenet-us-vm --zone="" --machine-type=e2-micro --subnet=privatesubnet-us
복사되었습니다.
출력은 다음과 같습니다.

Created [https://www.googleapis.com/compute/v1/projects/qwiklabs-gcp-04-972c7275ce91/zones/""/instances/privatenet-us-vm].
NAME: privatenet-us-vm
ZONE: ""
MACHINE_TYPE: e2-micro
PREEMPTIBLE:
INTERNAL_IP: 172.16.0.2
EXTERNAL_IP: 34.135.195.199
STATUS: RUNNING
완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. privatenet 네트워크에 VM 인스턴스가 정상적으로 생성되면 평가 점수가 표시됩니다.

privatenet-us-vm 인스턴스 만들기
다음 명령어를 실행하여 모든 VM 인스턴스의 목록을 출력합니다(영역 기준으로 정렬됨).

gcloud compute instances list --sort-by=ZONE
복사되었습니다.
출력은 다음과 같습니다.

NAME: mynet-eu-vm
ZONE: europe-west4-a
MACHINE_TYPE: e2-micro
PREEMPTIBLE:
INTERNAL_IP: 10.164.0.2
EXTERNAL_IP: 34.147.23.235
STATUS: RUNNING
NAME: mynet-us-vm
ZONE: ""
MACHINE_TYPE: e2-micro
PREEMPTIBLE:
INTERNAL_IP: 10.128.0.2
EXTERNAL_IP: 35.232.221.58
STATUS: RUNNING
...
Cloud 콘솔에서 탐색 메뉴(탐색 메뉴 아이콘) > Compute Engine > VM 인스턴스로 이동합니다.

Cloud 콘솔에 VM 인스턴스가 나열됩니다.

열 표시 옵션을 클릭하고 네트워크를 선택합니다. 확인을 클릭합니다.

us-east1에는 인스턴스가 3개, europe-west4에는 인스턴스가 1개 있습니다. 하지만 이러한 인스턴스는 3개의 VPC 네트워크(managementnet, mynetwork 및 privatenet)에 분산되어 있으며 다른 인스턴스와 같은 영역 또는 네트워크에 존재하는 경우는 없습니다. 다음 섹션에서는 이러한 점이 내부 연결성에 미치는 영향에 대해 살펴보겠습니다.

작업 3. VM 인스턴스 간의 연결성 살펴보기
VM 인스턴스 간의 연결성을 살펴봅니다. 구체적으로는 여러 VM 인스턴스가 동일한 영역에 존재하는 경우와 동일한 VPC 네트워크에 존재하는 경우 어떤 결과가 발생하는지 비교해 봅니다.

외부 IP 주소 핑하기
VM 인스턴스의 외부 IP 주소에 핑하여 공용 인터넷에서 인스턴스에 도달할 수 있는지 확인합니다.

Cloud 콘솔에서 탐색 메뉴 > Compute Engine > VM 인스턴스로 이동합니다.

mynet-eu-vm, managementnet-us-vm, privatenet-us-vm의 외부 IP 주소를 확인합니다.

mynet-us-vm에서 SSH를 클릭하여 터미널을 실행하고 연결합니다.

mynet-eu-vm의 외부 IP에 대한 연결을 테스트하려면 다음 명령어에 mynet-eu-vm의 외부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 mynet-eu-vm의 외부 IP 입력>
복사되었습니다.
명령어가 정상적으로 실행될 것입니다.

managementnet-us-vm의 외부 IP에 대한 연결을 테스트하려면 다음 명령어에 managementnet-us-vm의 외부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 managementnet-us-vm의 외부 IP 입력>
복사되었습니다.
명령어가 정상적으로 실행될 것입니다.

privatenet-us-vm의 외부 IP에 대한 연결을 테스트하려면 다음 명령어에 privatenet-us-vm의 외부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 privatenet-us-vm의 외부 IP 입력>
복사되었습니다.
명령어가 정상적으로 실행될 것입니다.

참고: 모든 VM 인스턴스의 외부 IP 주소에 핑할 수 있으며, 이는 인스턴스가 다른 영역이나 VPC 네트워크에 위치한 경우에도 마찬가지입니다. 이 사실은 이러한 인스턴스에 대한 공개 액세스가 앞서 설정한 ICMP 방화벽 규칙에 의해서만 제어된다는 점을 확인시켜 줍니다.
내부 IP 주소 핑하기
VM 인스턴스의 내부 IP 주소에 핑하여 VPC 네트워크 내의 인스턴스에 도달할 수 있는지 확인합니다.

Cloud 콘솔에서 탐색 메뉴 > Compute Engine > VM 인스턴스로 이동합니다.

mynet-eu-vm, managementnet-us-vm, privatenet-us-vm의 내부 IP 주소를 확인합니다.

mynet-us-vm의 SSH 터미널로 돌아갑니다.

mynet-eu-vm의 내부 IP에 대한 연결을 테스트하려면 다음 명령어에 mynet-eu-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 mynet-eu-vm의 내부 IP 입력>
복사되었습니다.
참고: mynet-eu-vm의 내부 IP 주소를 핑할 수 있는 이유는 해당 VM 인스턴스와 핑의 소스인 VM 인스턴스(mynet-us-vm)가 각기 다른 영역, 리전, 대륙에 존재하면서도 같은 VPC 네트워크상에 있기 때문입니다.
managementnet-us-vm의 내부 IP에 대한 연결을 테스트하려면 다음 명령어에 managementnet-us-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 managementnet-us-vm의 내부 IP 입력>
복사되었습니다.
참고: 이 명령어는 100% 패킷 손실이라고 표시되면서 정상적으로 실행되지 않아야 합니다.
privatenet-us-vm의 내부 IP에 대한 연결을 테스트하려면 다음 명령어에 privatenet-us-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 privatenet-us-vm의 내부 IP 입력>
복사되었습니다.
참고: 이 역시 100% 패킷 손실이라고 표시되면서 정상적으로 실행되지 않아야 합니다. managementnet-us-vm 및 privatenet-us-vm의 내부 IP 주소에 핑할 수 없는 이유는 해당 VM 인스턴스가 핑의 소스인 VM 인스턴스(mynet-us-vm)와 같은 us-east1 영역에 존재하기는 하지만 다른 VPC 네트워크상에 있기 때문입니다.
VPC 네트워크는 기본적으로 격리되어 있는 비공개 네트워크 도메인입니다. 하지만 VPC 피어링 또는 VPN과 같은 기능을 설정하지 않는 한, 네트워크 간 내부 IP 주소 통신은 허용되지 않습니다.

Which instance(s) should you be able to ping from mynet-us-vm using internal IP addresses?

mynet-eu-vm

privatenet-us-vm

managementnet-us-vm

작업 4. 다수의 네트워크 인터페이스가 있는 VM 인스턴스 만들기
VPC 네트워크의 모든 인스턴스에는 기본 네트워크 인터페이스가 있습니다. VM에 연결되는 네트워크 인터페이스를 추가로 만들 수도 있습니다. 다중 네트워크 인터페이스를 활용하면 인스턴스에서 다수의 VPC 네트워크(인터페이스 유형에 따라 최대 8개)에 직접 연결되는 구성을 만들 수 있습니다.

다수의 네트워크 인터페이스가 있는 VM 인스턴스 만들기
네트워크 인터페이스가 있는 vm-appliance 인스턴스를 privatesubnet-us, managementsubnet-us 및 mynetwork에 생성합니다. 이러한 서브넷의 CIDR 범위는 중복되지 않습니다. 이는 여러 NIC(네트워크 인터페이스 컨트롤러)가 있는 VM을 만들기 위한 필수 요건입니다.

Cloud 콘솔에서 탐색 메뉴 > Compute Engine > VM 인스턴스로 이동합니다.

인스턴스 만들기를 클릭합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
이름	vm-appliance
리전	us-east1
영역	<filled in at lab start>
시리즈	E2
머신 유형	e2-standard-4
참고: 인스턴스 1개에 허용되는 인터페이스의 개수는 인스턴스의 머신 유형과 vCPU의 개수에 따라 달라집니다. e2-standard-4는 최대 4개의 네트워크 인터페이스를 허용합니다. 자세한 내용은 Google Cloud 가이드의 네트워크 인터페이스의 최대 개수 섹션을 참조하세요.
고급 옵션에서 네트워킹, 디스크, 보안, 관리, 단독 테넌시 드롭다운을 클릭합니다.

네트워킹을 클릭합니다.

네트워크 인터페이스에서 드롭다운을 클릭하여 수정합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
네트워크	privatenet
서브네트워크	privatesubnet-us
완료를 클릭합니다.

네트워크 인터페이스 추가를 클릭합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
네트워크	managementnet
서브네트워크	managementsubnet-us
완료를 클릭합니다.

네트워크 인터페이스 추가를 클릭합니다.

다음 값을 설정하고 나머지 값은 모두 기본값으로 유지합니다.

속성	값(지정된 대로 값 입력 또는 옵션 선택)
네트워크	mynetwork
서브네트워크	mynetwork
완료를 클릭합니다.

만들기를 클릭합니다.

완료된 작업 테스트하기
진행 상황 확인하기를 클릭하여 실행한 작업을 확인합니다. 다수의 네트워크 인터페이스가 있는 VM 인스턴스가 정상적으로 생성되면 평가 점수가 표시됩니다.

다수의 네트워크 인터페이스가 있는 VM 인스턴스 만들기
네트워크 인터페이스 세부정보 살펴보기
Cloud 콘솔 및 VM 터미널에서 vm-appliance의 네트워크 인터페이스 세부정보를 살펴봅니다.

Cloud 콘솔에서 탐색 메뉴(탐색 메뉴 아이콘) > Compute Engine > VM 인스턴스로 이동합니다.
vm-appliance의 내부 IP 주소 내에서 nic0을 클릭하여 네트워크 인터페이스 세부정보 페이지를 엽니다.
nic0이 privatesubnet-us에 연결되어 있고, 해당 서브넷(172.16.0.0/24)에 내부 IP 주소가 할당되어 있고, 적용 가능한 방화벽 규칙이 있는지 확인합니다.
nic0을 클릭하고 nic1을 선택합니다.
nic1이 managementsubnet-us에 연결되어 있고, 해당 서브넷(10.130.0.0/20)에 내부 IP 주소가 할당되어 있고, 적용 가능한 방화벽 규칙이 있는지 확인합니다.
nic1을 클릭하고 nic2를 선택합니다.
nic2가 mynetwork에 연결되어 있고, 해당 서브넷(10.128.0.0/20)에 내부 IP 주소가 할당되어 있고, 적용 가능한 방화벽 규칙이 있는지 확인합니다.
참고: 각 네트워크 인터페이스에는 자체 내부 IP 주소가 있습니다. 따라서 VM 인스턴스가 이러한 네트워크와 통신할 수 있습니다.
Cloud 콘솔에서 탐색 메뉴 > Compute Engine > VM 인스턴스로 이동합니다.

vm-appliance의 경우 SSH를 클릭하여 터미널을 실행하고 연결합니다.

다음을 실행하여 VM 인스턴스 내의 네트워크 인터페이스를 나열합니다.

sudo ifconfig
복사되었습니다.
출력은 다음과 같습니다.

eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460
        inet 172.16.0.3  netmask 255.255.255.255  broadcast 172.16.0.3
        inet6 fe80::4001:acff:fe10:3  prefixlen 64  scopeid 0x20<link>
        ether 42:01:ac:10:00:03  txqueuelen 1000  (Ethernet)
        RX packets 626  bytes 171556 (167.5 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 568  bytes 62294 (60.8 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
eth1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460
        inet 10.130.0.3  netmask 255.255.255.255  broadcast 10.130.0.3
        inet6 fe80::4001:aff:fe82:3  prefixlen 64  scopeid 0x20<link>
        ether 42:01:0a:82:00:03  txqueuelen 1000  (Ethernet)
        RX packets 7  bytes 1222 (1.1 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 17  bytes 1842 (1.7 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
eth2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1460
        inet 10.128.0.3  netmask 255.255.255.255  broadcast 10.128.0.3
        inet6 fe80::4001:aff:fe80:3  prefixlen 64  scopeid 0x20<link>
        ether 42:01:0a:80:00:03  txqueuelen 1000  (Ethernet)
        RX packets 17  bytes 2014 (1.9 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 17  bytes 1862 (1.8 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
참고: sudo ifconfig 명령어는 Linux VM의 네트워크 인터페이스와 각 인터페이스의 내부 IT 주소를 출력합니다.
네트워크 인터페이스 연결성 살펴보기
서브넷의 VM 인스턴스에 핑하여 vm-appliance 인스턴스가 privatesubnet-us, managementsubnet-us 및 mynetwork에 연결되었음을 검증합니다.

Cloud 콘솔에서 탐색 메뉴 > Compute Engine > VM 인스턴스로 이동합니다.

privatenet-us-vm, managementnet-us-vm, mynet-us-vm 및 mynet-eu-vm의 내부 IP 주소를 확인합니다.

vm-appliance의 SSH 터미널로 돌아갑니다.

privatenet-us-vm의 내부 IP에 대한 연결을 테스트하려면 다음 명령어에 privatenet-us-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 privatenet-us-vm의 내부 IP 입력>
복사되었습니다.
정상적으로 실행됩니다.

다음을 실행하여 같은 테스트를 반복합니다.

ping -c 3 privatenet-us-vm
복사되었습니다.
참고: privatenet-us-vm에 이름으로 핑할 수 있는 이유는 인스턴스를 내부 IP 주소가 아닌 DNS 이름으로 처리할 수 있게 해주는 내부 DNS 서비스가 VPC 네트워크에 있기 때문입니다. 내부 DNS 쿼리가 인스턴스 호스트 이름으로 작성된 경우, 인스턴스의 기본 인터페이스(nic0)에서 확인합니다. 따라서 이 경우에는 privatenet-us-vm에 대해서만 정상 작동합니다.
managementnet-us-vm의 내부 IP에 대한 연결을 테스트하려면 다음 명령어에 managementnet-us-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 managementnet-us-vm의 내부 IP 입력>
복사되었습니다.
정상적으로 실행됩니다.

mynet-us-vm의 내부 IP에 대한 연결성을 테스트하려면 다음 명령어에 mynet-us-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 mynet-us-vm의 내부 IP 입력>
복사되었습니다.
정상적으로 실행됩니다.

mynet-eu-vm의 내부 IP에 대한 연결을 테스트하려면 다음 명령어에 mynet-eu-vm의 내부 IP를 삽입한 후 실행합니다.

ping -c 3 <여기에 mynet-eu-vm의 내부 IP 입력>
복사되었습니다.
참고: 이는 정상적으로 작동되지 않습니다. 다중 인터페이스 인스턴스에서 모든 인터페이스는 자신이 속한 서브넷의 경로를 가져옵니다. 또한 인스턴스는 기본 인터페이스인 eth0과 연결된 단일 기본 경로를 가져옵니다. 수동으로 달리 구성하지 않은 한, 직접 연결된 서브넷을 제외한 모든 대상을 향해 인스턴스에서 나가는 모든 트래픽은 eth0의 기본 경로를 통합니다.
vm-appliance 인스턴스의 경로 목록을 표시하려면 다음 명령어를 실행합니다.

ip route
복사되었습니다.
출력은 다음과 같습니다.

default via 172.16.0.1 dev eth0
10.128.0.0/20 via 10.128.0.1 dev eth2
10.128.0.1 dev eth2 scope link
10.130.0.0/20 via 10.130.0.1 dev eth1
10.130.0.1 dev eth1 scope link
172.16.0.0/24 via 172.16.0.1 dev eth0
172.16.0.1 dev eth0 scope link
참고: 기본 인터페이스 eth0에서는 기본 경로(172.16.0.1 dev eth0을 통한 기본값)를 가져오고 3개의 인터페이스(eth0, eth1, eth2)에서는 모두 각 서브넷의 경로를 가져옵니다. mynet-eu-vm(10.132.0.0/20)의 서브넷은 이 라우팅 테이블에 포함되어 있지 않으므로 해당 인스턴스에 대한 핑은 다른 VPC 네트워크에 있는 eth0의 vm-appliance에서 나갑니다. Google Cloud 가이드의 정책 라우팅 구성하기 섹션에 나와 있는 대로 정책 라우팅을 구성하여 동작을 변경할 수 있습니다.


 2. Hosting a Web App on GCP compute Engine


Task 1. Enable Compute Engine API
Next, enable the Compute Engine API.

Execute the following to enable the Compute Engine API:

gcloud services enable compute.googleapis.com
복사되었습니다.
Task 2. Create Cloud Storage bucket
You will use a Cloud Storage bucket to house your built code as well as your startup scripts.

From within Cloud Shell, execute the following to create a new Cloud Storage bucket:

gsutil mb gs://fancy-store-$DEVSHELL_PROJECT_ID
복사되었습니다.
Note: Use of the $DEVSHELL_PROJECT_ID environment variable within Cloud Shell is to help ensure the names of objects are unique. Since all Project IDs within Google Cloud must be unique, appending the Project ID should make other names unique as well.
Click Check my progress to verify the objective.
Create Cloud Storage bucket

Task 3. Clone source repository
You will be using the existing Fancy Store ecommerce website based on the monolith-to-microservices repository as the basis for your website.

You will clone the source code so you can focus on the aspects of deploying to Compute Engine. Later on in this lab, you will perform a small update to the code to demonstrate the simplicity of updating on Compute Engine.

Clone the source code and then navigate to the monolith-to-microservices directory:

git clone https://github.com/googlecodelabs/monolith-to-microservices.git
복사되었습니다.
cd ~/monolith-to-microservices
복사되었습니다.
Run the initial build of the code to allow the application to run locally:

./setup.sh
복사되었습니다.
It will take a few minutes for this script to finish.

Once completed, ensure Cloud Shell is running a compatible nodeJS version with the following command:

nvm install --lts
복사되었습니다.
Next, run the following to test the application, switch to the microservices directory, and start the web server:

cd microservices
npm start
복사되었습니다.
You should see the following output:

Products microservice listening on port 8082!
Frontend microservice listening on port 8080!
Orders microservice listening on port 8081!
Preview your application by clicking the web preview icon then selecting Preview on port 8080.
Web preview icon and Preview on port 8080 option highlighted

This opens a new window where you can see the frontend of Fancy Store.

Note: Within the Preview option, you will be able to see the Frontend; however, the Products and Orders functions will not work, as those services are not yet exposed.
Close this window after viewing the website and then press CTRL+C in the terminal window to stop the web server process.

Task 4. Create Compute Engine instances
Now it's time to start deploying some Compute Engine instances!

In the following steps you will:

Create a startup script to configure instances.

Clone source code and upload to Cloud Storage.

Deploy a Compute Engine instance to host the backend microservices.

Reconfigure the frontend code to utilize the backend microservices instance.

Deploy a Compute Engine instance to host the frontend microservice.

Configure the network to allow communication.

Create the startup script
A startup script will be used to instruct the instance what to do each time it is started. This way the instances are automatically configured.

Click Open Editor in the Cloud Shell ribbon to open the Code Editor.
Open Editor button

Navigate to the monolith-to-microservices folder.

Click on File > New File and create a file called startup-script.sh

Add the following code to the file. You will edit some of the code after it's added:

#!/bin/bash
# Install logging monitor. The monitor will automatically pick up logs sent to
# syslog.
curl -s "https://storage.googleapis.com/signals-agents/logging/google-fluentd-install.sh" | bash
service google-fluentd restart &
# Install dependencies from apt
apt-get update
apt-get install -yq ca-certificates git build-essential supervisor psmisc
# Install nodejs
mkdir /opt/nodejs
curl https://nodejs.org/dist/v16.14.0/node-v16.14.0-linux-x64.tar.gz | tar xvzf - -C /opt/nodejs --strip-components=1
ln -s /opt/nodejs/bin/node /usr/bin/node
ln -s /opt/nodejs/bin/npm /usr/bin/npm
# Get the application source code from the Google Cloud Storage bucket.
mkdir /fancy-store
gsutil -m cp -r gs://fancy-store-[DEVSHELL_PROJECT_ID]/monolith-to-microservices/microservices/* /fancy-store/
# Install app dependencies.
cd /fancy-store/
npm install
# Create a nodeapp user. The application will run as this user.
useradd -m -d /home/nodeapp nodeapp
chown -R nodeapp:nodeapp /opt/app
# Configure supervisor to run the node app.
cat >/etc/supervisor/conf.d/node-app.conf << EOF
[program:nodeapp]
directory=/fancy-store
command=npm start
autostart=true
autorestart=true
user=nodeapp
environment=HOME="/home/nodeapp",USER="nodeapp",NODE_ENV="production"
stdout_logfile=syslog
stderr_logfile=syslog
EOF
supervisorctl reread
supervisorctl update
복사되었습니다.
Find the text [DEVSHELL\_PROJECT\_ID\] in the file and replace it with the output from the following command:

echo $DEVSHELL_PROJECT_ID
복사되었습니다.
Example output:

qwiklabs-gcp-123456789xyz
The line of code within startup-script.sh should now be similar to the following:

gs://fancy-store-qwiklabs-gcp-123456789xyz/monolith-to-microservices/microservices/* /fancy-store/
복사되었습니다.
Save the file, then close it.

Cloud Shell Code Editor: Ensure "End of Line Sequence" is set to "LF" and not "CRLF". Check by looking at the bottom right of the Code Editor:

"End of Line Sequence"

If this is set to CRLF, click CRLF and then select LF in the drop down.
The startup script performs the following tasks:

Installs the Logging agent. The agent automatically collects logs from syslog.

Installs Node.js and Supervisor. Supervisor runs the app as a daemon.

Clones the app's source code from Cloud Storage Bucket and installs dependencies.

Configures Supervisor to run the app. Supervisor makes sure the app is restarted if it exits unexpectedly or is stopped by an admin or process. It also sends the app's stdout and stderr to syslog for the Logging agent to collect.

Run the following to copy the startup-script.sh file into your bucket:

gsutil cp ~/monolith-to-microservices/startup-script.sh gs://fancy-store-$DEVSHELL_PROJECT_ID
복사되었습니다.
It will now be accessible at: https://storage.googleapis.com/[BUCKET_NAME]/startup-script.sh.

[BUCKET_NAME] represents the name of the Cloud Storage bucket. This will only be viewable by authorized users and service accounts by default, so inaccessible through a web browser. Compute Engine instances will automatically be able to access this through their service account.

Copy code into the Cloud Storage bucket
When instances launch, they pull code from the Cloud Storage bucket, so you can store some configuration variables within the .env file of the code.

Note: You could also code this to pull environment variables from elsewhere, but for demonstration purposes this is a simple method to handle configuration. In production, environment variables would likely be stored outside of the code.
Copy the cloned code into your bucket:

cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/
복사되었습니다.
Note: The node_modules dependencies directories are deleted to ensure the copy is as fast and efficient as possible. These are recreated on the instances when they start up.
Click Check my progress to verify the objective.
Copy startup script and code to Cloud Storage bucket

Deploy the backend instance
The first instance to be deployed will be the backend instance which will house the Orders and Products microservices.

Note: In a production environment, you may want to separate each microservice into their own instance and instance group to allow them to scale independently. For demonstration purposes, both backend microservices (Orders & Products) will reside on the same instance and instance group.
Execute the following command to create an n1-standard-1 instance that is configured to use the startup script. It is tagged as a backend instance so you can apply specific firewall rules to it later:

gcloud compute instances create backend \
    --machine-type=n1-standard-1 \
    --tags=backend \
   --metadata=startup-script-url=https://storage.googleapis.com/fancy-store-$DEVSHELL_PROJECT_ID/startup-script.sh
복사되었습니다.
Note: If you are asked to specify a zone, ensure a default zone was configured within the Set Up portion of this lab.
Configure a connection to the backend
Before you deploy the frontend of the application, you need to update the configuration to point to the backend you just deployed.

Retrieve the external IP address of the backend with the following command, look under the EXTERNAL_IP tab for the backend instance:

gcloud compute instances list
복사되었습니다.
Example output:

NAME     ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP   STATUS
backend  us-central1-f  n1-standard-1                   10.128.0.2   34.68.223.88  RUNNING
Copy the External IP for the backend.

In the Cloud Shell Explorer, navigate to monolith-to-microservices > react-app.

In the Code Editor, select View > Toggle Hidden Files in order to see the .env file.

Edit the .env file to point to the External IP of the backend. [BACKEND_ADDRESS] represents the External IP address of the backend instance determined from the above gcloud command.

In the .env file, replace localhost with your [BACKEND_ADDRESS]:

REACT_APP_ORDERS_URL=http://[BACKEND_ADDRESS]:8081/api/orders
REACT_APP_PRODUCTS_URL=http://[BACKEND_ADDRESS]:8082/api/products
복사되었습니다.
Save the file.

Rebuild react-app, which will update the frontend code:

cd ~/monolith-to-microservices/react-app
npm install && npm run-script build
복사되었습니다.
Then copy the application code into the Cloud Storage bucket:

cd ~
rm -rf monolith-to-microservices/*/node_modules
복사되었습니다.
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/
복사되었습니다.
Deploy the frontend instance
Now that the code is configured, deploy the frontend instance.

Execute the following to deploy the frontend instance with a similar command as before. This instance is tagged as frontend for firewall purposes:

gcloud compute instances create frontend \
    --machine-type=n1-standard-1 \
    --tags=frontend \
    --metadata=startup-script-url=https://storage.googleapis.com/fancy-store-$DEVSHELL_PROJECT_ID/startup-script.sh
복사되었습니다.
Note: The deployment command and startup script is used with both the frontend and backend instances for simplicity, and because the code is configured to launch all microservices by default. As a result, all microservices run on both the frontend and backend in this sample. In a production environment you'd only run the microservices you need on each component.
Configure the network
Create firewall rules to allow access to port 8080 for the frontend, and ports 8081-8082 for the backend. These firewall commands use the tags assigned during instance creation for application:

gcloud compute firewall-rules create fw-fe \
    --allow tcp:8080 \
    --target-tags=frontend
복사되었습니다.
gcloud compute firewall-rules create fw-be \
    --allow tcp:8081-8082 \
    --target-tags=backend
복사되었습니다.
The website should now be fully functional.

In order to navigate to the external IP of the frontend, you need to know the address. Run the following and look for the EXTERNAL_IP of the frontend instance:

gcloud compute instances list
복사되었습니다.
Example output:

NAME      ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
backend   us-central1-f  n1-standard-1               10.128.0.2   35.184.46.126   RUNNING
frontend  us-central1-f  n1-standard-1               10.128.0.3   35.223.110.167  RUNNING
It may take a couple minutes for the instance to start and be configured.

Wait 30 seconds, then execute the following to monitor for the application becoming ready, replacing FRONTEND_ADDRESS with the External IP for the frontend instance:

watch -n 2 curl http://[FRONTEND_ADDRESS]:8080
복사되었습니다.
Once you see output similar to the following, the website should be ready..

Output

Press CTRL+C to cancel the watch command

Open a new browser tab and browse to http://[FRONTEND_ADDRESS]:8080 to access the website, where [FRONTEND_ADDRESS] is the frontend EXTERNAL_IP determined above.

Try navigating to the Products and Orders pages; these should now work.

Fancy Store Products tabbed page. Product images are tiled.

Click Check my progress to verify the objective.
Deploy instances and configure network

Task 5. Create managed instance groups
To allow the application to scale, managed instance groups will be created and will use the frontend and backend instances as Instance Templates.

A managed instance group (MIG) contains identical instances that you can manage as a single entity in a single zone. Managed instance groups maintain high availability of your apps by proactively keeping your instances available, that is, in the RUNNING state. We will be using managed instance groups for our frontend and backend instances to provide autohealing, load balancing, autoscaling, and rolling updates.

Create instance template from source instance
Before you can create a managed instance group, you have to first create an instance template that will be the foundation for the group. Instance templates allow you to define the machine type, boot disk image or container image, network, and other instance properties to use when creating new VM instances. You can use instance templates to create instances in a managed instance group or even to create individual instances.

To create the instance template, use the existing instances you created previously.

First, stop both instances:

gcloud compute instances stop frontend
복사되었습니다.
gcloud compute instances stop backend
복사되었습니다.
Then, create the instance template from each of the source instances:

gcloud compute instance-templates create fancy-fe \
    --source-instance=frontend
복사되었습니다.
gcloud compute instance-templates create fancy-be \
    --source-instance=backend
복사되었습니다.
Confirm the instance templates were created:

gcloud compute instance-templates list
복사되었습니다.
Example output:

NAME      MACHINE_TYPE  PREEMPTIBLE  CREATION_TIMESTAMP
fancy-be  n1-standard-1                  2020-02-03T10:34:12.966-08:00
fancy-fe  n1-standard-1                   2020-02-03T10:34:01.082-08:00
With the instance templates created, delete the backend vm to save resource space:

gcloud compute instances delete backend
복사되었습니다.
Type and enter y when prompted.
Normally, you could delete the frontend vm as well, but you will use it to update the instance template later in the lab.

Create managed instance group
Next, create two managed instance groups, one for the frontend and one for the backend:

gcloud compute instance-groups managed create fancy-fe-mig \
    --base-instance-name fancy-fe \
    --size 2 \
    --template fancy-fe
복사되었습니다.
gcloud compute instance-groups managed create fancy-be-mig \
    --base-instance-name fancy-be \
    --size 2 \
    --template fancy-be
복사되었습니다.
These managed instance groups will use the instance templates and are configured for two instances each within each group to start. The instances are automatically named based on the base-instance-name specified with random characters appended.

For your application, the frontend microservice runs on port 8080, and the backend microservice runs on port 8081 for orders and port 8082 for products:

gcloud compute instance-groups set-named-ports fancy-fe-mig \
    --named-ports frontend:8080
복사되었습니다.
gcloud compute instance-groups set-named-ports fancy-be-mig \
    --named-ports orders:8081,products:8082
복사되었습니다.
Since these are non-standard ports, you specify named ports to identify these. Named ports are key:value pair metadata representing the service name and the port that it's running on. Named ports can be assigned to an instance group, which indicates that the service is available on all instances in the group. This information is used by the HTTP Load Balancing service that will be configured later.

Configure autohealing
To improve the availability of the application itself and to verify it is responding, configure an autohealing policy for the managed instance groups.

An autohealing policy relies on an application-based health check to verify that an app is responding as expected. Checking that an app responds is more precise than simply verifying that an instance is in a RUNNING state, which is the default behavior.

Note: Separate health checks for load balancing and for autohealing will be used. Health checks for load balancing can and should be more aggressive because these health checks determine whether an instance receives user traffic. You want to catch non-responsive instances quickly so you can redirect traffic if necessary.

In contrast, health checking for autohealing causes Compute Engine to proactively replace failing instances, so this health check should be more conservative than a load balancing health check.
Create a health check that repairs the instance if it returns "unhealthy" 3 consecutive times for the frontend and backend:

gcloud compute health-checks create http fancy-fe-hc \
    --port 8080 \
    --check-interval 30s \
    --healthy-threshold 1 \
    --timeout 10s \
    --unhealthy-threshold 3
복사되었습니다.
gcloud compute health-checks create http fancy-be-hc \
    --port 8081 \
    --request-path=/api/orders \
    --check-interval 30s \
    --healthy-threshold 1 \
    --timeout 10s \
    --unhealthy-threshold 3
복사되었습니다.
Create a firewall rule to allow the health check probes to connect to the microservices on ports 8080-8081:

gcloud compute firewall-rules create allow-health-check \
    --allow tcp:8080-8081 \
    --source-ranges 130.211.0.0/22,35.191.0.0/16 \
    --network default
복사되었습니다.
Apply the health checks to their respective services:

gcloud compute instance-groups managed update fancy-fe-mig \
    --health-check fancy-fe-hc \
    --initial-delay 300
복사되었습니다.
gcloud compute instance-groups managed update fancy-be-mig \
    --health-check fancy-be-hc \
    --initial-delay 300
복사되었습니다.
Note: It can take 15 minutes before autohealing begins monitoring instances in the group.
Continue with the lab to allow some time for autohealing to monitor the instances in the group. You will simulate a failure to test the autohealing at the end of the lab.
Click Check my progress to verify the objective.
Create managed instance groups

Task 6. Create load balancers
To complement our managed instance groups, you will be using an HTTP(S) Load Balancers to serve traffic to the frontend and backend microservices, and using mappings to send traffic to the proper backend services based on pathing rules. This will expose a single load balanced IP for all services.

You can learn more about the Load Balancing options on Google Cloud: Overview of Load Balancing.

Create HTTP(S) load balancer
Google Cloud offers many different types of load balancers. For this lab you use an HTTP(S) Load Balancer for your traffic. An HTTP load balancer is structured as follows:

A forwarding rule directs incoming requests to a target HTTP proxy.
The target HTTP proxy checks each request against a URL map to determine the appropriate backend service for the request.
The backend service directs each request to an appropriate backend based on serving capacity, zone, and instance health of its attached backends. The health of each backend instance is verified using an HTTP health check. If the backend service is configured to use an HTTPS or HTTP/2 health check, the request will be encrypted on its way to the backend instance.
Sessions between the load balancer and the instance can use the HTTP, HTTPS, or HTTP/2 protocol. If you use HTTPS or HTTP/2, each instance in the backend services must have an SSL certificate.
Note: For demonstration purposes in order to avoid SSL certificate complexity, use HTTP instead of HTTPS. For production, it is recommended to use HTTPS for encryption wherever possible.
Create health checks that will be used to determine which instances are capable of serving traffic for each service:

gcloud compute http-health-checks create fancy-fe-frontend-hc \
  --request-path / \
  --port 8080
복사되었습니다.
gcloud compute http-health-checks create fancy-be-orders-hc \
  --request-path /api/orders \
  --port 8081
복사되었습니다.
gcloud compute http-health-checks create fancy-be-products-hc \
  --request-path /api/products \
  --port 8082
복사되었습니다.
Note: These health checks are for the load balancer, and only handle directing traffic from the load balancer; they do not cause the managed instance groups to recreate instances.
Create backend services that are the target for load-balanced traffic. The backend services will use the health checks and named ports you created:

gcloud compute backend-services create fancy-fe-frontend \
  --http-health-checks fancy-fe-frontend-hc \
  --port-name frontend \
  --global
복사되었습니다.
gcloud compute backend-services create fancy-be-orders \
  --http-health-checks fancy-be-orders-hc \
  --port-name orders \
  --global
복사되었습니다.
gcloud compute backend-services create fancy-be-products \
  --http-health-checks fancy-be-products-hc \
  --port-name products \
  --global
복사되었습니다.
Add the Load Balancer's backend services:

gcloud compute backend-services add-backend fancy-fe-frontend \
  --instance-group fancy-fe-mig \
  --instance-group-zone us-central1-f \
  --global
복사되었습니다.
gcloud compute backend-services add-backend fancy-be-orders \
  --instance-group fancy-be-mig \
  --instance-group-zone us-central1-f \
  --global
복사되었습니다.
gcloud compute backend-services add-backend fancy-be-products \
  --instance-group fancy-be-mig \
  --instance-group-zone us-central1-f \
  --global
복사되었습니다.
Create a URL map. The URL map defines which URLs are directed to which backend services:

gcloud compute url-maps create fancy-map \
  --default-service fancy-fe-frontend
복사되었습니다.
Create a path matcher to allow the /api/orders and /api/products paths to route to their respective services:

gcloud compute url-maps add-path-matcher fancy-map \
   --default-service fancy-fe-frontend \
   --path-matcher-name orders \
   --path-rules "/api/orders=fancy-be-orders,/api/products=fancy-be-products"
복사되었습니다.
Create the proxy which ties to the URL map:

gcloud compute target-http-proxies create fancy-proxy \
  --url-map fancy-map
복사되었습니다.
Create a global forwarding rule that ties a public IP address and port to the proxy:

gcloud compute forwarding-rules create fancy-http-rule \
  --global \
  --target-http-proxy fancy-proxy \
  --ports 80
복사되었습니다.
Click Check my progress to verify the objective.
Create HTTP(S) load balancers

Update the configuration
Now that you have a new static IP address, update the code on the frontend to point to this new address instead of the ephemeral address used earlier that pointed to the backend instance.

In Cloud Shell, change to the react-app folder which houses the .env file that holds the configuration:

cd ~/monolith-to-microservices/react-app/
복사되었습니다.
Find the IP address for the Load Balancer:

gcloud compute forwarding-rules list --global
복사되었습니다.
Example output:

NAME                    REGION  IP_ADDRESS     IP_PROTOCOL  TARGET
fancy-http-rule          34.102.237.51  TCP          fancy-proxy
Return to the Cloud Shell Editor and edit the .env file again to point to Public IP of Load Balancer. [LB_IP] represents the External IP address of the backend instance determined above.

REACT_APP_ORDERS_URL=http://[LB_IP]/api/orders
REACT_APP_PRODUCTS_URL=http://[LB_IP]/api/products
복사되었습니다.
Note: The ports are removed in the new address because the load balancer is configured to handle this forwarding for you.
Save the file.

Rebuild react-app, which will update the frontend code:

cd ~/monolith-to-microservices/react-app
npm install && npm run-script build
복사되었습니다.
Copy the application code into your bucket:

cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/
복사되었습니다.
Update the frontend instances
Now that there is new code and configuration, you want the frontend instances within the managed instance group to pull the new code.

Since your instances pull the code at startup, you can issue a rolling restart command:

gcloud compute instance-groups managed rolling-action replace fancy-fe-mig \
    --max-unavailable 100%
복사되었습니다.
Note: In this example of a rolling replace, you specifically state that all machines can be replaced immediately through the --max-unavailable parameter. Without this parameter, the command would keep an instance alive while restarting others to ensure availability. For testing purposes, you specify to replace all immediately for speed.
Click Check my progress to verify the objective.
Update the frontend instances

Test the website
Wait approximately 30 seconds after issues the rolling-action replace command in order to give the instances time to be processed, and then check the status of the managed instance group until instances appear in the list:

watch -n 2 gcloud compute instance-groups list-instances fancy-fe-mig
복사되었습니다.
Once items appear in the list, exit the watch command by pressing CTRL+C.

Run the following to confirm the service is listed as HEALTHY:

watch -n 2 gcloud compute backend-services get-health fancy-fe-frontend --global
복사되었습니다.
Wait until the 2 services are listed as HEALTHY.
Example output:

---
backend: https://www.googleapis.com/compute/v1/projects/my-gce-codelab/zones/us-central1-a/instanceGroups/fancy-fe-mig
status:
  healthStatus:
  - healthState: HEALTHY
    instance: https://www.googleapis.com/compute/v1/projects/my-gce-codelab/zones/us-central1-a/instances/fancy-fe-x151
    ipAddress: 10.128.0.7
    port: 8080
  - healthState: HEALTHY
    instance: https://www.googleapis.com/compute/v1/projects/my-gce-codelab/zones/us-central1-a/instances/fancy-fe-cgrt
    ipAddress: 10.128.0.11
    port: 8080
  kind: compute#backendServiceGroupHealth
Note: If one instance encounters an issue and is UNHEALTHY it should automatically be repaired. Wait for this to happen. If neither instance enters a HEALTHY state after waiting a little while, something is wrong with the setup of the frontend instances that accessing them on port 8080 doesn't work. Test this by browsing to the instances directly on port 8080.
Once both items appear as HEALTHY on the list, exit the watch command by pressing CTRL+C.
Note: The application will be accessible via http://[LB_IP] where [LB_IP] is the IP_ADDRESS specified for the Load Balancer, which can be found with the following command:
gcloud compute forwarding-rules list --global

You'll be checking the application later in the lab.
Task 7. Scaling Compute Engine
So far, you have created two managed instance groups with two instances each. This configuration is fully functional, but a static configuration regardless of load. Next you will create an autoscaling policy based on utilization to automatically scale each managed instance group.

Automatically resize by utilization
To create the autoscaling policy, execute the following:

gcloud compute instance-groups managed set-autoscaling \
  fancy-fe-mig \
  --max-num-replicas 2 \
  --target-load-balancing-utilization 0.60
복사되었습니다.
gcloud compute instance-groups managed set-autoscaling \
  fancy-be-mig \
  --max-num-replicas 2 \
  --target-load-balancing-utilization 0.60
복사되었습니다.
These commands create an autoscaler on the managed instance groups that automatically adds instances when utilization is above 60% utilization, and removes instances when the load balancer is below 60% utilization.

Enable content delivery network
Another feature that can help with scaling is to enable a Content Delivery Network service, to provide caching for the frontend.

Execute the following command on the frontend service:

gcloud compute backend-services update fancy-fe-frontend \
    --enable-cdn --global
복사되었습니다.
When a user requests content from the HTTP(S) load balancer, the request arrives at a Google Front End (GFE) which first looks in the Cloud CDN cache for a response to the user's request. If the GFE finds a cached response, the GFE sends the cached response to the user. This is called a cache hit.

If the GFE can't find a cached response for the request, the GFE makes a request directly to the backend. If the response to this request is cacheable, the GFE stores the response in the Cloud CDN cache so that the cache can be used for subsequent requests.

Click Check my progress to verify the objective.
Scaling Compute Engine

Task 8. Update the website
Updating instance template
Existing instance templates are not editable; however, since your instances are stateless and all configuration is done through the startup script, you only need to change the instance template if you want to change the template settings . Now you're going to make a simple change to use a larger machine type and push that out.

Update the frontend instance, which acts as the basis for the instance template. During the update, you will put a file on the updated version of the instance template's image, then update the instance template, roll out the new template, and then confirm the file exists on the managed instance group instances.

Now modify the machine type of your instance template, by switching from the n1-standard-1 machine type into a custom machine type with 4 vCPU and 3840MiB RAM.

Run the following command to modify the machine type of the frontend instance:

gcloud compute instances set-machine-type frontend --machine-type custom-4-3840
복사되었습니다.
Create the new Instance Template:

gcloud compute instance-templates create fancy-fe-new \
    --source-instance=frontend \
    --source-instance-zone=us-central1-f
복사되었습니다.
Roll out the updated instance template to the Managed Instance Group:

gcloud compute instance-groups managed rolling-action start-update fancy-fe-mig \
    --version template=fancy-fe-new
복사되었습니다.
Wait 30 seconds then run the following to monitor the status of the update:

watch -n 2 gcloud compute instance-groups managed list-instances fancy-fe-mig
복사되었습니다.
This will take a few moments.

Once you have at least 1 instance in the following condition:

STATUS: RUNNING

ACTION set to None

INSTANCE_TEMPLATE: the new template name (fancy-fe-new)

Copy the name of one of the machines listed for use in the next command.

CTRL+C to exit the watch process.

Run the following to see if the virtual machine is using the new machine type (custom-4-3840), where [VM_NAME] is the newly created instance:

gcloud compute instances describe [VM_NAME] | grep machineType
복사되었습니다.
Expected example output:

machineType: https://www.googleapis.com/compute/v1/projects/project-name/zones/us-central1-f/machineTypes/custom-4-3840
Make changes to the website
Scenario: Your marketing team has asked you to change the homepage for your site. They think it should be more informative of who your company is and what you actually sell.

Task: Add some text to the homepage to make the marketing team happy! It looks like one of the developers has already created the changes with the file name index.js.new. You can just copy this file to index.js and the changes should be reflected. Follow the instructions below to make the appropriate changes.

Run the following commands to copy the updated file to the correct file name:

cd ~/monolith-to-microservices/react-app/src/pages/Home
mv index.js.new index.js
복사되었습니다.
Print the file contents to verify the changes:

cat ~/monolith-to-microservices/react-app/src/pages/Home/index.js
복사되었습니다.
The resulting code should look like this:

/*
Copyright 2019 Google LLC
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    https://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
import React from "react";
import { makeStyles } from "@material-ui/core/styles";
import Paper from "@material-ui/core/Paper";
import Typography from "@material-ui/core/Typography";
const useStyles = makeStyles(theme => ({
  root: {
    flexGrow: 1
  },
  paper: {
    width: "800px",
    margin: "0 auto",
    padding: theme.spacing(3, 2)
  }
}));
export default function Home() {
  const classes = useStyles();
  return (
    <div>


          Fancy Fashion &amp; Style Online
        


          Tired of mainstream fashion ideas, popular trends and societal norms?
          This line of lifestyle products will help you catch up with the Fancy trend and express your personal style.
          Start shopping Fancy items now!
        

</div>
  );
}
You updated the React components, but you need to build the React app to generate the static files.

Run the following command to build the React app and copy it into the monolith public directory:

cd ~/monolith-to-microservices/react-app
npm install && npm run-script build
복사되었습니다.
Then re-push this code to the bucket:

cd ~
rm -rf monolith-to-microservices/*/node_modules
gsutil -m cp -r monolith-to-microservices gs://fancy-store-$DEVSHELL_PROJECT_ID/
복사되었습니다.
Push changes with rolling replacements
Now force all instances to be replaced to pull the update:

gcloud compute instance-groups managed rolling-action replace fancy-fe-mig \
    --max-unavailable=100%
복사되었습니다.
Note: In this example of a rolling replace, you specifically state that all machines can be replaced immediately through the --max-unavailable parameter. Without this parameter, the command would keep an instance alive while replacing others.

For testing purposes, you specify to replace all immediately for speed. In production, leaving a buffer would allow the website to continue serving the website while updating.
Click Check my progress to verify the objective.
Update the website

Wait approximately 30 seconds after issuing the rolling-action replace command in order to give the instances time to be processed, and then check the status of the managed instance group until instances appear in the list:

watch -n 2 gcloud compute instance-groups list-instances fancy-fe-mig
복사되었습니다.
Once items appear in the list, exit the watch command by pressing CTRL+C.

Run the following to confirm the service is listed as HEALTHY:

watch -n 2 gcloud compute backend-services get-health fancy-fe-frontend --global
복사되었습니다.
Wait a few moments for both services to appear and become HEALTHY.
Example output:

---
backend: https://www.googleapis.com/compute/v1/projects/my-gce-codelab/zones/us-central1-a/instanceGroups/fancy-fe-mig
status:
  healthStatus:
  - healthState: HEALTHY
    instance: https://www.googleapis.com/compute/v1/projects/my-gce-codelab/zones/us-central1-a/instances/fancy-fe-x151
    ipAddress: 10.128.0.7
    port: 8080
  - healthState: HEALTHY
    instance: https://www.googleapis.com/compute/v1/projects/my-gce-codelab/zones/us-central1-a/instances/fancy-fe-cgrt
    ipAddress: 10.128.0.11
    port: 8080
  kind: compute#backendServiceGroupHealth
Once items appear in the list, exit the watch command by pressing CTRL+C.

Browse to the website via http://[LB_IP] where [LB_IP] is the IP_ADDRESS specified for the Load Balancer, which can be found with the following command:

gcloud compute forwarding-rules list --global
복사되었습니다.
The new website changes should now be visible.

Simulate failure
In order to confirm the health check works, log in to an instance and stop the services.

To find an instance name, execute the following:

gcloud compute instance-groups list-instances fancy-fe-mig
복사되었습니다.
Copy an instance name, then run the following to secure shell into the instance, where INSTANCE_NAME is one of the instances from the list:

gcloud compute ssh [INSTANCE_NAME]
복사되었습니다.
Type in "y" to confirm, and press Enter twice to not use a password.

Within the instance, use supervisorctl to stop the application:

sudo supervisorctl stop nodeapp; sudo killall node
복사되었습니다.
Exit the instance:

exit
복사되었습니다.
Monitor the repair operations:

watch -n 2 gcloud compute operations list \
--filter='operationType~compute.instances.repair.*'
복사되었습니다.
This will take a few minutes to complete.

Look for the following example output:

NAME                                                  TYPE                                       TARGET                                 HTTP_STATUS  STATUS  TIMESTAMP
repair-1568314034627-5925f90ee238d-fe645bf0-7becce15  compute.instances.repair.recreateInstance  us-central1-a/instances/fancy-fe-1vqq  200          DONE    2019-09-12T11:47:14.627-07:00
The managed instance group recreated the instance to repair it.

You can also monitor through the Console - go to Navigation menu > Compute Engine > VM instances.






























