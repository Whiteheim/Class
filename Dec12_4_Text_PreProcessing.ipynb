{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYVM13HdKB715Qoe7V/GUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Whiteheim/WH/blob/main/Dec12_4_Text_PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 표제어 추출 (Lemmatization)\n",
        "\n",
        "말뭉치(코퍼스)의 단어 개수를 줄일 수 있는 기법\n",
        "\n",
        "be 동사 : be, am, are, is\n",
        "\n",
        "공부하다 : 공부하고, 공부때문에, 공부여서, 공부덕분에, ...\n",
        "\n",
        "* 분석시에 단어 빈도수 기반으로 진행 -> 자연어 처리 단계에서 상당히 자주 사용\n",
        "\n",
        "* 형태소로부터 단어를 만들어가는 것 : 형태학\n",
        "  * 어간(stem) : 의미가 있는 단어의 핵심부분\n",
        "  * 접사(affix) : 단어에 추가적인 의미를 부여하는 부분 \n",
        "\n",
        "  형태학적 파싱 : 코퍼스에서 어간과 접사를 분리하는 것\n",
        "\n",
        "  ex) students => student + s"
      ],
      "metadata": {
        "id": "e9YD1Nr7j29_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeIXRo-kjrab",
        "outputId": "33e335c9-b01f-4037-bf22-2aa965cf181f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk # 자연어 처리를 위한 패키지\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNetLemmatizer : NLTK에서 지원하는 표제어 추출도구\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sphJDNfKlGIG",
        "outputId": "b0e3d3a6-36d1-4edc-b3c6-d0f322a185c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['sky', 'computer', 'having', 'lives', 'love', 'mouse', 'dies', 'listend', 'ate', 'has']\n",
        "\n",
        "print('추출 전 : ', words)\n",
        "print('추출 후 : ', [lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckh0Ovh_lh7M",
        "outputId": "af10118b-0c91-41ba-e250-c58968499e74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "추출 전 :  ['sky', 'computer', 'having', 'lives', 'love', 'mouse', 'dies', 'listend', 'ate', 'has']\n",
            "추출 후 :  ['sky', 'computer', 'having', 'life', 'love', 'mouse', 'dy', 'listend', 'ate', 'ha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('dies', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bq0IzpT9mmXL",
        "outputId": "a6450886-c968-4fe9-cf11-84a56e4855b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('listend', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YSfnOpB-msuz",
        "outputId": "b6036f08-fef0-43a4-f91e-4a584a6f5376"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'listend'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('better', 'a')\n",
        "\n",
        "# v : 동사 / a : 형용사 / n : 명사 / r : 부사"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FDIvCrJImxDT",
        "outputId": "cfc87c5c-f524-431e-b928-b2edee6df96f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 어간 추출 (Stemming)\n",
        "\n"
      ],
      "metadata": {
        "id": "VN0eABCQnFzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivbWJBL8nAaj",
        "outputId": "bd526fba-5b80-4ac1-9eec-c949f49addad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "akjMafsSnRMp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"\n",
        "wraps up its second season on Sunday, and you know what that means: It’s time for somebody to die. Multiple somebodies, actually. As resort manager Valentina (Sabrina Impacciatore) learns from her unwitting romantic rival, Rocco (Federico Ferrante), in the flash-forward that opens the premiere, at least one guest has drowned—and there are “other bodies” somewhere, too. With the apparent exception of Daphne (Meghann Fahy), who is shown discovering a corpse floating in the Ionian Sea, victims could include just about any vacationer and a few of their Italian associates.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79nyqYuNncZj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "words2 = word_tokenize(sentence)\n",
        "print(words2)\n",
        "print([stemmer.stem(w) for w in words2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-fkoc7NnqWq",
        "outputId": "18c002f2-d894-4a3e-a971-56e53d0b5eae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wraps', 'up', 'its', 'second', 'season', 'on', 'Sunday', ',', 'and', 'you', 'know', 'what', 'that', 'means', ':', 'It', '’', 's', 'time', 'for', 'somebody', 'to', 'die', '.', 'Multiple', 'somebodies', ',', 'actually', '.', 'As', 'resort', 'manager', 'Valentina', '(', 'Sabrina', 'Impacciatore', ')', 'learns', 'from', 'her', 'unwitting', 'romantic', 'rival', ',', 'Rocco', '(', 'Federico', 'Ferrante', ')', ',', 'in', 'the', 'flash-forward', 'that', 'opens', 'the', 'premiere', ',', 'at', 'least', 'one', 'guest', 'has', 'drowned—and', 'there', 'are', '“', 'other', 'bodies', '”', 'somewhere', ',', 'too', '.', 'With', 'the', 'apparent', 'exception', 'of', 'Daphne', '(', 'Meghann', 'Fahy', ')', ',', 'who', 'is', 'shown', 'discovering', 'a', 'corpse', 'floating', 'in', 'the', 'Ionian', 'Sea', ',', 'victims', 'could', 'include', 'just', 'about', 'any', 'vacationer', 'and', 'a', 'few', 'of', 'their', 'Italian', 'associates', '.']\n",
            "['wrap', 'up', 'it', 'second', 'season', 'on', 'sunday', ',', 'and', 'you', 'know', 'what', 'that', 'mean', ':', 'it', '’', 's', 'time', 'for', 'somebodi', 'to', 'die', '.', 'multipl', 'somebodi', ',', 'actual', '.', 'as', 'resort', 'manag', 'valentina', '(', 'sabrina', 'impacciator', ')', 'learn', 'from', 'her', 'unwit', 'romant', 'rival', ',', 'rocco', '(', 'federico', 'ferrant', ')', ',', 'in', 'the', 'flash-forward', 'that', 'open', 'the', 'premier', ',', 'at', 'least', 'one', 'guest', 'ha', 'drowned—and', 'there', 'are', '“', 'other', 'bodi', '”', 'somewher', ',', 'too', '.', 'with', 'the', 'appar', 'except', 'of', 'daphn', '(', 'meghann', 'fahi', ')', ',', 'who', 'is', 'shown', 'discov', 'a', 'corps', 'float', 'in', 'the', 'ionian', 'sea', ',', 'victim', 'could', 'includ', 'just', 'about', 'ani', 'vacation', 'and', 'a', 'few', 'of', 'their', 'italian', 'associ', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PorterStemmer : 알고리즘 (사람이 만든 라이브러리)\n",
        "\n",
        "규칙 기반의 접근 => 어림짐작하는 작업 => 섬세한 작업 X => 사전에 없는 단어가 도출 될 수도 있음\n",
        "* 마틴포터 홈페이지에서 다양한 것들을 살펴볼 수 있음\n",
        "* 규칙 기반의 접근\n",
        "  * ~ALIZE => AL\n",
        "  * ~ANCE => 삭제 \n",
        "  * ~ICAL => IC"
      ],
      "metadata": {
        "id": "C5HJhvCloM_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = ['channelize', 'allowance', 'typical']\n",
        "\n",
        "print('추출 전 : ', word)\n",
        "print('추출 후 : ', [stemmer.stem(w) for w in word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaFZw9a6o7fC",
        "outputId": "035892bf-4a8f-4116-f3a9-34753ca523d8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "추출 전 :  ['channelize', 'allowance', 'typical']\n",
            "추출 후 :  ['channel', 'allow', 'typic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "uMKhLJkyt-X5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()"
      ],
      "metadata": {
        "id": "wi8hGnNfuIKm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([stemmer.stem(w) for w in words])\n",
        "print()\n",
        "print([lancaster.stem(w) for w in words])\n",
        "\n",
        "# 두 스태머가 각각 다른 결과를 보여줌\n",
        "# -> 서로 다른 알고리즘을 사용하기 때문\n",
        "# 제대로 된 표제어를 뽑아오지는 못하고 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i6jmVtnuLxG",
        "outputId": "c79c710e-e9cd-43d5-d34e-79928b19e3b0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sky', 'comput', 'have', 'live', 'love', 'mous', 'die', 'listend', 'ate', 'ha']\n",
            "\n",
            "['sky', 'comput', 'hav', 'liv', 'lov', 'mous', 'die', 'listend', 'at', 'has']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 불용어(Stopword)\n",
        "\n",
        "단어들 중에서 의미가 없는 단어\n",
        "\n",
        "데이터 중에서 의미가 있는 단어 토근만 취급하기 위해 의미를 가지지 않은 단어들을 제거하는 작업"
      ],
      "metadata": {
        "id": "EcEN7GJRutVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4vilEcxvAHG",
        "outputId": "aef0b76a-50ee-4692-c2f1-2a4daf67d373"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "LgF-la7hvGll"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에 있는 불용어 \n",
        "s = stopwords.words('english')\n",
        "print(len(s))\n",
        "print(s[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ_KlRZsvLAF",
        "outputId": "374215cb-a1dc-40e5-eb6a-50f98040f583"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"\n",
        "wraps up its second season on Sunday, and you know what that means: \n",
        "It’s time for somebody to die. Multiple somebodies, actually. \n",
        "As resort manager Valentina (Sabrina Impacciatore) learns from her unwitting romantic rival, \n",
        "Rocco (Federico Ferrante), in the flash-forward that opens the premiere, \n",
        "at least one guest has drowned—and there are “other bodies” somewhere, too. \n",
        "With the apparent exception of Daphne (Meghann Fahy), \n",
        "who is shown discovering a corpse floating in the Ionian Sea, \n",
        "victims could include just about any vacationer and a few of their Italian associates.\n",
        "\"\"\"\n",
        "\n",
        "# NLTK에서 지정한 불용어를 가져오기\n",
        "sw = stopwords.words('english')\n",
        "# 문장을 단어로 쪼개는 작업\n",
        "# lcst = LancasterStemmer()\n",
        "# words = [lcst.stem(w) for w in words]\n",
        "word = word_tokenize(sentence)\n",
        "# 불용어가 아닌 단어들만 list에 담아서 출력\n",
        "# for w in words:\n",
        "#   for s in range(len(sw)):\n",
        "#     if w == sw[s]:\n",
        "#       del words[s]\n",
        "result = []\n",
        "for w in word:\n",
        "  if w not in sw:\n",
        "    result.append(w)\n",
        "\n",
        "print(word)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTaWkDCZvn97",
        "outputId": "6da3dec2-8013-4b71-ee37-6057aae7fd03"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wraps', 'up', 'its', 'second', 'season', 'on', 'Sunday', ',', 'and', 'you', 'know', 'what', 'that', 'means', ':', 'It', '’', 's', 'time', 'for', 'somebody', 'to', 'die', '.', 'Multiple', 'somebodies', ',', 'actually', '.', 'As', 'resort', 'manager', 'Valentina', '(', 'Sabrina', 'Impacciatore', ')', 'learns', 'from', 'her', 'unwitting', 'romantic', 'rival', ',', 'Rocco', '(', 'Federico', 'Ferrante', ')', ',', 'in', 'the', 'flash-forward', 'that', 'opens', 'the', 'premiere', ',', 'at', 'least', 'one', 'guest', 'has', 'drowned—and', 'there', 'are', '“', 'other', 'bodies', '”', 'somewhere', ',', 'too', '.', 'With', 'the', 'apparent', 'exception', 'of', 'Daphne', '(', 'Meghann', 'Fahy', ')', ',', 'who', 'is', 'shown', 'discovering', 'a', 'corpse', 'floating', 'in', 'the', 'Ionian', 'Sea', ',', 'victims', 'could', 'include', 'just', 'about', 'any', 'vacationer', 'and', 'a', 'few', 'of', 'their', 'Italian', 'associates', '.']\n",
            "['wraps', 'second', 'season', 'Sunday', ',', 'know', 'means', ':', 'It', '’', 'time', 'somebody', 'die', '.', 'Multiple', 'somebodies', ',', 'actually', '.', 'As', 'resort', 'manager', 'Valentina', '(', 'Sabrina', 'Impacciatore', ')', 'learns', 'unwitting', 'romantic', 'rival', ',', 'Rocco', '(', 'Federico', 'Ferrante', ')', ',', 'flash-forward', 'opens', 'premiere', ',', 'least', 'one', 'guest', 'drowned—and', '“', 'bodies', '”', 'somewhere', ',', '.', 'With', 'apparent', 'exception', 'Daphne', '(', 'Meghann', 'Fahy', ')', ',', 'shown', 'discovering', 'corpse', 'floating', 'Ionian', 'Sea', ',', 'victims', 'could', 'include', 'vacationer', 'Italian', 'associates', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한글 불용어 제거하기\n",
        "* 토큰화 -> 조사 or 접속사 같이 명사 or 형용사에서 필요없는 단어들을 제거하는 형태\n",
        "* 한국어의 경우에는 사용자가 직접 불용어를 지정해서 사용하는 경우가 많음\n"
      ],
      "metadata": {
        "id": "ouInswwk5L9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrMDNXTJ5ZnP",
        "outputId": "eebee481-0db8-4f3e-f717-adb7eeb34239"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (4.9.1)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from Konlpy) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->Konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->Konlpy) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "ttwOnWCB5fCP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "ex = \"점심 먹고 나서 피곤하시죠? 여러분은 어떤 메뉴를 드셨나요? 저는 샐러드를 먹었습니다.\"\n",
        "sw = \"를 어떤 는 은\"\n",
        "\n",
        "sw = set(sw.split(' '))\n",
        "token = okt.morphs(ex) # 형태소 분석\n",
        "\n",
        "result = [w for w in token if w not in sw]\n",
        "\n",
        "print(token) # 불용어 제거 전\n",
        "print()\n",
        "print(result) # 불용어 제거 후"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYuXmE1-5in2",
        "outputId": "547711f1-b364-449c-8dc9-aec7150e47a4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['점심', '먹고', '나서', '피곤하시죠', '?', '여러분', '은', '어떤', '메뉴', '를', '드셨나요', '?', '저', '는', '샐러드', '를', '먹었습니다', '.']\n",
            "\n",
            "['점심', '먹고', '나서', '피곤하시죠', '?', '여러분', '메뉴', '드셨나요', '?', '저', '샐러드', '먹었습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정수 인코딩 (Integer Encoding)\n",
        "\n",
        "컴퓨터의 입장에서 텍스트보다는 숫자를 더 쉽게 처리하는 경향이 있음 \n",
        "\n",
        "텍스트에 정수를 부여하는 방법\n",
        "  1. 단어를 빈도수를 기준으로 정렬\n",
        "  2. 정렬된 단어집합 구성\n",
        "  3. 빈도가 높은순 -> 낮은순으로 숫자를 부여\n",
        "  "
      ],
      "metadata": {
        "id": "U-e4uMcE6gPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 동요\n",
        "text = \"\"\"Daddy finger, daddy finger, where are you?\n",
        "Here I am, here I am\n",
        "How do you do?\n",
        "Mommy finger, Mommy finger, where are you?\n",
        "Here I am, here I am\n",
        "How do you do?\n",
        "Brother finger, Brother finger, where are you?\n",
        "Here I am, here I am\n",
        "How do you do?\n",
        "Sister finger, Sister finger, where are you?\n",
        "Here I am, here I am\n",
        "How do you do?\n",
        "Baby finger, Baby finger, where are you?\n",
        "Here I am, here I am\n",
        "How do you do?\n",
        "Yeap\n",
        "Finger family , Finger family, where are you?\n",
        "Here we are, here we are\n",
        "How do you do?\n",
        "Hey\n",
        "\"\"\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "uRVQfFSU66Vr",
        "outputId": "aa85924e-73a9-4e64-b3a4-25b6b35dd864"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Daddy finger, daddy finger, where are you?\\nHere I am, here I am\\nHow do you do?\\nMommy finger, Mommy finger, where are you?\\nHere I am, here I am\\nHow do you do?\\nBrother finger, Brother finger, where are you?\\nHere I am, here I am\\nHow do you do?\\nSister finger, Sister finger, where are you?\\nHere I am, here I am\\nHow do you do?\\nBaby finger, Baby finger, where are you?\\nHere I am, here I am\\nHow do you do?\\nYeap\\nFinger family , Finger family, where are you?\\nHere we are, here we are\\nHow do you do?\\nHey\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize # 영어 문장 토큰화\n",
        "from nltk.tokenize import word_tokenize # 영어 단어 토큰화\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "b7hceNQp8JBF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentence = sent_tokenize(text)\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMDuPivP8boV",
        "outputId": "fd3d7c2e-491f-4b6b-a88e-c1ce4b4d6e49"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Daddy finger, daddy finger, where are you?',\n",
              " 'Here I am, here I am\\nHow do you do?',\n",
              " 'Mommy finger, Mommy finger, where are you?',\n",
              " 'Here I am, here I am\\nHow do you do?',\n",
              " 'Brother finger, Brother finger, where are you?',\n",
              " 'Here I am, here I am\\nHow do you do?',\n",
              " 'Sister finger, Sister finger, where are you?',\n",
              " 'Here I am, here I am\\nHow do you do?',\n",
              " 'Baby finger, Baby finger, where are you?',\n",
              " 'Here I am, here I am\\nHow do you do?',\n",
              " 'Yeap\\nFinger family , Finger family, where are you?',\n",
              " 'Here we are, here we are\\nHow do you do?',\n",
              " 'Hey']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화 -> 불용어를 뺀 단어 토큰들을 list에 담기\n",
        "\n",
        "sw = set(stopwords.words('english'))\n",
        "final_sentence = []\n",
        "aa = {}\n",
        "\n",
        "for s in sentence:\n",
        "  # 단어 토큰화 \n",
        "  word = word_tokenize(s)\n",
        "  result = []\n",
        "  for w in word:\n",
        "    w = w.lower() # 모든 단어를 소문자화 -> 단어 개수를 줄이는데에 도움\n",
        "    if w not in sw:\n",
        "      if len(w) > 2:       # 단어 길이가 1인것들 (관사 같은것) 잘라내기\n",
        "        result.append(w)\n",
        "        if w not in aa:\n",
        "          aa[w] = 0\n",
        "        aa[w] += 1\n",
        "  final_sentence.append(result)\n",
        "print(final_sentence)\n",
        "print(aa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkzQH6xI8qq0",
        "outputId": "8040755a-fb77-4dd7-fc41-4052dacc541c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['daddy', 'finger', 'daddy', 'finger'], [], ['mommy', 'finger', 'mommy', 'finger'], [], ['brother', 'finger', 'brother', 'finger'], [], ['sister', 'finger', 'sister', 'finger'], [], ['baby', 'finger', 'baby', 'finger'], [], ['yeap', 'finger', 'family', 'finger', 'family'], [], ['hey']]\n",
            "{'daddy': 2, 'finger': 12, 'mommy': 2, 'brother': 2, 'sister': 2, 'baby': 2, 'yeap': 1, 'family': 2, 'hey': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'hey' 단어의 빈도수\n",
        "print(aa['hey'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCJ2lpvbB2xJ",
        "outputId": "ebdb8c8a-734a-4951-c255-b30af685cc94"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sorted() : 빈도수대로 정렬\n",
        "# sorted(정렬할 데이터, key 옵션, reverse옵션)\n",
        "#   key옵션 : key parameter\n",
        "#             어떤것을 기준으로 정렬할지 (key에 준 값으로 정렬)\n",
        "#   reverse옵션 : False(default) >> 오름차순 \n",
        "\n",
        "# sort() vs sorted():\n",
        "#   sort()는 리스트 자체를 정렬하여 바꾸는 형태\n",
        "#   sorted()는 리스트 원본은 유지한 채, 새로운 리스트에 정렬한 값을 입력 \n",
        "\n",
        "# key=lambda x : x[1] => x[1]의 값이 정렬의 기준 -> 빈도수를 기준으로 정렬\n",
        "\n",
        "aaSort = sorted(aa.items(), key=lambda x : x[1], reverse=True)\n",
        "print(aaSort)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYALaqSMCDmB",
        "outputId": "82a347f8-b2fa-444e-c572-89f8d0803241"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('finger', 12), ('daddy', 2), ('mommy', 2), ('brother', 2), ('sister', 2), ('baby', 2), ('family', 2), ('yeap', 1), ('hey', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [높은 빈도수]를 가지고 있는 단어일수록 [낮은 정수값]을 부여 (정수는 1부터 부여)\n",
        "\n",
        "# 빈도수가 1이하인것들은 삭제(결과에 나오지 않도록)\n",
        "# {'finger' : 1, 'daddy' : 2, 'mommy' : 3, ...}\n",
        "\n",
        "aa_index = {}\n",
        "i = 0\n",
        "\n",
        "# aaSort2 = sorted(aa.items(), key=lambda x : x[1], reverse=True)\n",
        "# for a in aaSort2:\n",
        "#   if a.values() != 1:\n",
        "#     rank.append(a[a])\n",
        "for (word, frequency) in aaSort:\n",
        "  if frequency > 1:\n",
        "    i = i + 1\n",
        "    aa_index[word] = i\n",
        "\n",
        "print(aa_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd5Cm14aDNnR",
        "outputId": "b75b4ed8-3511-484c-ce48-cf2b1ffb827c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'finger': 1, 'daddy': 2, 'mommy': 3, 'brother': 4, 'sister': 5, 'baby': 6, 'family': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 빈도수가 가장 높은 상위 5개 출력\n",
        "# aaSort2  = sorted(aa.items(), key=lambda x : (x[1], 5), reverse=True)\n",
        "# print(aaSort2)\n",
        "freSize = 5\n",
        "# 인덱스가 5초과 (6이상)인 단어들을 aa_final이라는 변수에 담기\n",
        "aa_final = [w for (w, index) in aa_index.items() if index >= freSize + 1]\n",
        "\n",
        "for w in aa_final:\n",
        "  del aa_index[w]\n",
        "\n",
        "print(aa_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-LVlyZ6GBqW",
        "outputId": "aff9d803-7b59-4157-a7aa-fe43797d0aac"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'finger': 1, 'daddy': 2, 'mommy': 3, 'brother': 4, 'sister': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ['finger, 'daddy', mommy, 'brother', 'sister'] >> aa_index에 더이상 존재하지 않는 단어 추가\n",
        "# [1, 2, 3, 4, 5, ?]\n",
        "# Out_of_Vocabulary : 단어 집합에 없는 단어 => OOV\n",
        "# aa_index에 'OOV'라는 단어가 있는 자리를 하나 만들고, 그 단어 집합에 존재하지 않는 단어를\n",
        "#   OOV의 인덱스로 인코딩"
      ],
      "metadata": {
        "id": "C1PLQdoTIPSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa_index['OOV'] = len(aa_index) + 1\n",
        "print(aa_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKUMEqDAI9SI",
        "outputId": "75152798-90f2-4813-f3b6-9a8fe3ec499d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'finger': 1, 'daddy': 2, 'mommy': 3, 'brother': 4, 'sister': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장마다 텍스트 대신 그 자리에 해당하는 인덱스로 변환\n",
        "# 문장마다 단어로 토큰화 : final_sentence\n",
        "\n",
        "encoding_sentences = []\n",
        "for fs in final_sentence:\n",
        "  encoding_sentence = []\n",
        "  for w in fs:\n",
        "    try:\n",
        "      # 단어 집합에 있는 단어면 해당 단어의 정수값을 넣어줌\n",
        "      encoding_sentence.append(aa_index[w])\n",
        "    except KeyError:\n",
        "      # 단어 집합에 없는 단어면 OOV의 정수값을 넣어줌\n",
        "      encoding_sentence.append(aa_index['OOV'])\n",
        "  encoding_sentences.append(encoding_sentence)\n",
        "print(final_sentence)\n",
        "print(encoding_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytUZdM83JGwl",
        "outputId": "ac6ab940-5453-4f29-fee4-ee85ecb2874f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['daddy', 'finger', 'daddy', 'finger'], [], ['mommy', 'finger', 'mommy', 'finger'], [], ['brother', 'finger', 'brother', 'finger'], [], ['sister', 'finger', 'sister', 'finger'], [], ['baby', 'finger', 'baby', 'finger'], [], ['yeap', 'finger', 'family', 'finger', 'family'], [], ['hey']]\n",
            "[[2, 1, 2, 1], [], [3, 1, 3, 1], [], [4, 1, 4, 1], [], [5, 1, 5, 1], [], [6, 1, 6, 1], [], [6, 1, 6, 1, 6], [], [6]]\n"
          ]
        }
      ]
    }
  ]
}